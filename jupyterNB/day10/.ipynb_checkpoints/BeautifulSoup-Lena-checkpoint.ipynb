{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "## An introduction to Webscraping\n",
    "### By Lena Hajjar, with additions by Jeff Parker\n",
    "\n",
    "<p> Many of you have expressed interest in processing information found on websites</p>\n",
    "<p> There are many options, but the library that is mentioned most often is Beautiful Soup</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Basics\n",
    "Beautiful Soup has an excellent website, with full documentation.  See https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \n",
    "\n",
    "<b> NOTE: </b> Webscraping may be against the terms and conditions of some websites (Yelp, FB, etc). Sometimes it's because they offer the services for a fee (Twitter, Facebook, etc) - so if you really want the information and really don't want to break the terms & conditions (or get your IP address banned from the website), you can try getting the information through the recommended sources. The rules are really made to prevent people from doing mass scrapings for profit - I doubt they would bother tracking down the graduate student scraping one or two pages for educational purposes. \n",
    "\n",
    "<b> robots.txt </b> The convention is that the publisher will list the rules in a file called robots.txt at the root of the website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /scripts/\n",
      "Disallow: /cgi-bin/\n",
      "Disallow: /community/admin/\n",
      "Disallow: /community/images/\n",
      "Disallow: /community/mod/\n",
      "Disallow: /community/avatar.php\n",
      "Disallow: /community/editpost.php\n",
      "Disallow: /community/member2.php\n",
      "Disallow: /community/moderator.php\n",
      "Disallow: /community/newreply.php\n",
      "Disallow: /community/newthread.php\n",
      "Disallow: /community/online.php\n",
      "Disallow: /community/postings.php\n",
      "Disallow: /community/private.php\n",
      "Disallow: /community/register.php\n",
      "Disallow: /community/report.php\n",
      "Disallow: /community/threadrate.php\n",
      "Disallow: /community/usercp.php\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# first, we want to read the rules of the road\n",
    "webpage = requests.get(\"https://www.movieforums.com/robots.txt\")\n",
    "webtext = webpage.text\n",
    "print(webtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is our use legitimate?\n",
    "What does that mean for us?  Is this use legitimate?  We can use the robotparser library to check.  \n",
    "\n",
    "We demonstrate below, checking if we can scrape the Extension School site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(\"https://www.extension.harvard.edu/robots.txt\")\n",
    "rp.read()\n",
    "print(rp.can_fetch(\"*\", \"https://www.extension.harvard.edu/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start at the beginning:\n",
    "Let's say we want to look at a thread in movieforums.com about the movie Die Hard.  Here is a thread we will process: https://www.movieforums.com/community/showthread.php?t=49209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# first, we should request the webpage\n",
    "webpage = requests.get(\"https://www.movieforums.com/community/showthread.php?t=49209\")\n",
    "webtext = webpage.text\n",
    "# We can print it, but it isn't very useful\n",
    "print(webtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(webtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have the ingredients - let's make the soup!\n",
    "soup = BeautifulSoup(webtext, \"html.parser\") \n",
    "# I just passed the string with all of the html data, and asked to use the html parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#you can ask beautiful soup to print everything it parsed\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the organization of a webpage?\n",
    "from IPython.display import Image\n",
    "Image('http://www.openbookproject.net/tutorials/getdown/css/images/lesson4/HTMLDOMTree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool! we have our soup- now let's see what's in it. \n",
    "# You can look for a tag itself (like the title tag)\n",
    "print(soup.title) #notice that the tags are still here\n",
    "print(\"===========================\")\n",
    "print(soup.title.string) #cool, now we know how to get the text inbetween the tags!\n",
    "print(\"===========================\")\n",
    "print(soup.div)\n",
    "print(\"===========================\")\n",
    "print(soup.div['class']) #you can ask for an attribute \n",
    "print(\"===========================\")\n",
    "print(soup.a)\n",
    "print(\"===========================\")\n",
    "print(soup.a['href']) #cool, a new link! \n",
    "\n",
    "# note that this only returns the first instance of each tag specified! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we wanted ALL links?\n",
    "links = soup.find_all('a')\n",
    "print(links) # this looks kind of ugly. What's happening here?\n",
    "print(\"===========================\")\n",
    "print(type(links)) #it's a set!\n",
    "print(\"===========================\")\n",
    "\n",
    "# say I wanted to find out all the links and text displayed\n",
    "for i in links:\n",
    "     print(i['href'], \":\", i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got a key error! I should probably deal with that error\n",
    "\n",
    "for i in links:\n",
    "    try:\n",
    "        print(i['href'], \":\", i.string)\n",
    "    except KeyError:\n",
    "        pass # I don't care mwhat happens to links without 'href's, so I skip them        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could also get all the text on the page (problem solved, right?)\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with this \"solution\":\n",
    "<ul>\n",
    "<li> It's ugly </li>\n",
    "<li> Everything is smushed together - you don't know who said what or when. </li>\n",
    "<li> You literally have all the text including..\n",
    "<ul>\n",
    "<li>quotes (that'd be repeated text)</li> \n",
    "<li>signatures (meaningless to the question) </li>\n",
    "<li>and all the text that is a part of the webpage! </li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<p> So it looks like what we really want to do is find each post, and then get the information we want from it</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## after doing some detective work, we discovered that posts are found in div tags with class 'post'.\n",
    "\n",
    "allDiv = soup.find_all('div') # all div's\n",
    "postDivs = [] # empty list\n",
    "postDateDivs = []\n",
    "postTextDivs = []\n",
    "\n",
    "for div in allDiv:\n",
    "    try:\n",
    "        if 'post' in div.get('class'): # if it's a post, add to post list\n",
    "            postDivs.append(div)\n",
    "        elif 'postdate' in div.get('class'): # if it's a date, add to date list\n",
    "            postDateDivs.append(div)\n",
    "        elif 'posttext' in div.get('class'): # if it's text, add to text list\n",
    "            postTextDivs.append(div)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postTextDivs.pop(1) # the advertisement is always the second post\n",
    "postDivs.pop(1) # the advertisement is always the second post\n",
    "\n",
    "print(len(postDivs))\n",
    "print(len(postDateDivs))\n",
    "print(len(postTextDivs))\n",
    "print(postDivs)\n",
    "print(\"+++++++++++++\")\n",
    "print(postDateDivs)\n",
    "print(\"+++++++++++++\")\n",
    "print(postTextDivs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Awesome,</b> we now have all the posts. Now we need to figure out what information we actually want from the posts. \n",
    "For the sake of doing all of this to get something meaningful, I've decided that my goal is to get the following:\n",
    "<ul>\n",
    "<li>Username</li>\n",
    "<li>TimeStamp</li>\n",
    "<li>Post</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting with the username. \n",
    "## After some sluething, we found out that usernames are in the a tag with class 'username'\n",
    "## Let's make this a function!\n",
    "\n",
    "def findUserName(postDiv):\n",
    "    allPostLinks = postDiv.find_all('a')\n",
    "    # iterate through all links\n",
    "    for link in allPostLinks:\n",
    "        try:\n",
    "            if 'username' in link.get('class'): # if class type is username, get the name!\n",
    "                return link.text\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "## test...\n",
    "for post in postDivs:\n",
    "    print(findUserName(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's find the timestamp\n",
    "\n",
    "def findTimeStamp(postDateDiv):\n",
    "    try: # we know the time stamp is the title of date items\n",
    "        return postDateDiv.a['title']\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "for div in postDateDivs:\n",
    "    print(findTimeStamp(div))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now for the actual text..\n",
    "\n",
    "def findText(postTextDiv):\n",
    "    \"\"\" This function finds all text, and all the text that was quoted from someplace else\n",
    "        It finds the text, erases all quoted text, and then returns the remaining, original text.\"\"\"\n",
    "    allText = None \n",
    "    quotedText = None\n",
    "    postDivs = postTextDiv.find_all('div')\n",
    "    #find text\n",
    "    for div in postDivs:\n",
    "        try: \n",
    "            if \"post_message\" in div['id']:\n",
    "                allText = div.get_text()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try: #if any text was quoted, we need to save it so we can remove it from all text\n",
    "            if \"quotecontainer\" in div.get('class'):\n",
    "                quotedText = div.get_text()\n",
    "                #print(quotedText)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # print(allText, \"\\n\", quotedText)\n",
    "    # if the post isn't quoting anything, return the post text\n",
    "    if quotedText == None: \n",
    "        allText = allText.replace(\",\", \"\")\n",
    "        return allText\n",
    "    else: #otherwise you need to remove the quoted text \n",
    "        allText = allText.replace(quotedText,\"\")\n",
    "        allText = allText.replace(\",\", \"\")\n",
    "        return allText\n",
    "    \n",
    "for div in postTextDivs:\n",
    "    print(findText(div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## okay! Now that we know how to find all the info we want, we can make a csv file! \n",
    "allUserNames  = [findUserName(div) for div in postDivs]\n",
    "allTimeStamps = [findTimeStamp(div) for div in postDateDivs]\n",
    "allText       = [findText(div) for div in postTextDivs]\n",
    "print(len(allUserNames), len(allTimeStamps), len(allText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now write to csv file..\n",
    "import csv\n",
    "\n",
    "allRows = []\n",
    "for username, timestamp, text in zip(allUserNames, allTimeStamps, allText):\n",
    "    allRows.append([username, timestamp, text.strip()])\n",
    "\n",
    "#print(allRows)\n",
    "with open(\"DieHardData2.csv\", \"a\", encoding = \"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"UserName\", \"TimeStamp\", \"Text\"])\n",
    "    for row in allRows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look in your directory to for the csv file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the next page? The link to the next page is indcated with a '>>'\n",
    "\n",
    "def findNextPage(allLinks):\n",
    "    for link in allLinks:\n",
    "        if link.get_text() == \"Â»\":\n",
    "            return \"https://www.movieforums.com/community/\"+ link['href']\n",
    "    ## if we get all the way to end and didn't find it, assume it's the end of the forum\n",
    "    return None\n",
    "\n",
    "print(findNextPage(links))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cool! We got all the pieces ~ now we should put them together into a nice, neat set of funcitons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeForum(url, fileName):\n",
    "    \"\"\" This takes a forum from movieforum.com and scrapes all entries. \n",
    "        It then creates a file with the Usernames, TimeStamp, and Text of each entry. \"\"\"\n",
    "    \n",
    "    ## first, open the page:\n",
    "    webpage = requests.get(url)\n",
    "    webtext = webpage.text\n",
    "    \n",
    "    ## now let's make some soup\n",
    "    soup = BeautifulSoup(webtext, \"html.parser\") \n",
    "    \n",
    "    ## get links for future reference\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    ## now find all the right div sections:\n",
    "    allDiv       = soup.find_all('div') #all div's\n",
    "    postDivs     = [] #empty list\n",
    "    postDateDivs = []\n",
    "    postTextDivs = []\n",
    "\n",
    "    for div in allDiv:\n",
    "        try:\n",
    "            if 'post' in div.get('class'):\n",
    "                postDivs.append(div)\n",
    "            elif 'postdate' in div.get('class'):\n",
    "                postDateDivs.append(div)\n",
    "            elif 'posttext' in div.get('class'):\n",
    "                postTextDivs.append(div)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    ## we already described the functions to find the username, timestamp, and text, so let's use them!!\n",
    "    allUserNames = [findUserName(div) for div in postDivs]\n",
    "    allTimeStamps = [findTimeStamp(div) for div in postDateDivs]\n",
    "    allText = [findText(div) for div in postTextDivs]\n",
    "    \n",
    "    ## create the rows for the csv file\n",
    "    allRows = []\n",
    "    for username, timestamp, text in zip(allUserNames, allTimeStamps, allText):\n",
    "        allRows.append([username, timestamp, text.strip()])\n",
    "    \n",
    "    ## print to file! \n",
    "    with open(fileName, \"a\", encoding = \"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"UserName\", \"TimeStamp\", \"Text\"])\n",
    "        for row in allRows:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    ## almost done - now we should check for a next page. If there is, call this function again.\n",
    "    ## If not, then we can just call it a day. \n",
    "    \n",
    "    nextPage = findNextPage(links)\n",
    "    if nextPage != None:\n",
    "        scrapeForum(nextPage, fileName) ## since we open the fileName using 'a', it'll just append to the end\n",
    "   \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeForum(\"https://www.movieforums.com/community/showthread.php?t=49209\", \"DieHardForum2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeForum(\"https://www.movieforums.com/community/showthread.php?t=45018\", \"pirate2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "<p> We can successfully scrape a whole web forum! </p>\n",
    "<p> Some things to remember </p>\n",
    "<ul> \n",
    "<li> This will ONLY work for movieforums.com. We other website may not have the same tags </li>\n",
    "<li> It takes a bit of effort to make the script, but it's WORTH IT </li>\n",
    "<li> This script may have to be changed over time.  Websites get updated, and so do the HMTL tags in it </li>\n",
    "<li> Beautiful Soup does not work for websites written with javascript. </li>\n",
    "<li> Overall, it's pretty cool!</li>\n",
    "<ul>\n",
    "\n",
    "\n",
    "\n",
    "## Thanks, Lena!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
